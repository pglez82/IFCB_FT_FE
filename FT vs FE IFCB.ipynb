{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Experimento\n",
    "\n",
    "El objetivo de este notebook es comparar el finetuning (FT) y el featureextraction (FE) sobre el problema del plankton: finetuning entrena la red completa y featureextraction solo toca la capa final. \n",
    "\n",
    "Un segundo objetivo es comparar que cantidad de datos necesitamos con cada una de las dos tećnicas.\n",
    "\n",
    "Realizaremos las pruebas utilizando un conjunto de entrenamiento con los años 2006 y 2007 y el test con 2008.\n",
    "\n",
    "El objetivo no es obtener el máximo acierto sino ver que método es mejor según la cantidad de datos que tengamos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os,sys\n",
    "\n",
    "if not os.path.isdir(\"/media/nas/pgonzalez/IFCB_HDF5\"):\n",
    "    print(\"You should have the IFCB_HDF5 project in this directory to run this notebook\")\n",
    "    raise StopExecution\n",
    "sys.path.insert(1, os.path.abspath(\"/media/nas/pgonzalez/IFCB_HDF5\"))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carga de datos\n",
    "Leemos los metadatos y si no existen en local los descargamos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import pandas as pd\n",
    "\n",
    "if not os.path.isfile('IFCB.csv.zip'):\n",
    "    print(\"CSV data do not exist. Downloading...\")\n",
    "    !wget -O IFCB.csv.zip \"https://unioviedo-my.sharepoint.com/:u:/g/personal/gonzalezgpablo_uniovi_es/EfsVLhFsYJpPjO0KZlpWUq0BU6LaqJ989Re4XzatS9aG4Q?download=1\"\n",
    "\n",
    "data = pd.read_csv('IFCB.csv.zip',compression='infer', header=0,sep=',',quotechar='\"')\n",
    "#Compute sample and year information\n",
    "data['year'] = data['Sample'].str[6:10].astype(str) #Compute the year\n",
    "samples=data.groupby('Sample').first()\n",
    "samples=samples[['year']]\n",
    "print(data)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                        Sample  roi_number        OriginalClass  \\\n",
      "0        IFCB1_2006_158_000036           1                  mix   \n",
      "1        IFCB1_2006_158_000036           2  Tontonia_gracillima   \n",
      "2        IFCB1_2006_158_000036           3                  mix   \n",
      "3        IFCB1_2006_158_000036           4                  mix   \n",
      "4        IFCB1_2006_158_000036           5                  mix   \n",
      "...                        ...         ...                  ...   \n",
      "3457814  IFCB5_2014_353_205141        6850       Leptocylindrus   \n",
      "3457815  IFCB5_2014_353_205141        6852                  mix   \n",
      "3457816  IFCB5_2014_353_205141        6855                  mix   \n",
      "3457817  IFCB5_2014_353_205141        6856                  mix   \n",
      "3457818  IFCB5_2014_353_205141        6857                  mix   \n",
      "\n",
      "              AutoClass FunctionalGroup  year  \n",
      "0                   mix      Flagellate  2006  \n",
      "1           ciliate_mix         Ciliate  2006  \n",
      "2                   mix      Flagellate  2006  \n",
      "3                   mix      Flagellate  2006  \n",
      "4                   mix      Flagellate  2006  \n",
      "...                 ...             ...   ...  \n",
      "3457814  Leptocylindrus          Diatom  2014  \n",
      "3457815             mix      Flagellate  2014  \n",
      "3457816             mix      Flagellate  2014  \n",
      "3457817             mix      Flagellate  2014  \n",
      "3457818             mix      Flagellate  2014  \n",
      "\n",
      "[3457819 rows x 6 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filtrado de datos\n",
    "Quitamos ejemplos de las clases (pasandolos a mix), de cuatro clases que no existen en el train pero si en el test (Odontella, Hemiaulus, Gonyaulax y Stephanopyxis)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "classcolumn = \"AutoClass\" #Autoclass means 51 classes\n",
    "yearstraining = ['2006','2007'] #Years to consider as training\n",
    "yearsvalidation = ['2008'] #Years to consider as test\n",
    "\n",
    "samplestraining = list(samples[samples['year'].isin(yearstraining)].index) #Samples to consider for training\n",
    "samplesvalidation = list(samples[samples['year'].isin(yearsvalidation)].index) #Samples to consider for testing\n",
    "\n",
    "data[classcolumn]\n",
    "\n",
    "classes=np.unique(data[classcolumn])\n",
    "classes.sort()\n",
    "\n",
    "cls_to_delete=np.argwhere((classes=='Odontella') | (classes=='Hemiaulus') | (classes=='Gonyaulax') | (classes=='Stephanopyxis'))\n",
    "classes=np.delete(classes,cls_to_delete)\n",
    "print(classes)\n",
    "\n",
    "#Check data by year\n",
    "print(pd.crosstab(index=data['year'],columns='count'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Asterionellopsis' 'Cerataulina' 'Ceratium' 'Chaetoceros' 'Corethron'\n",
      " 'Coscinodiscus' 'Cylindrotheca' 'DactFragCerataul' 'Dactyliosolen'\n",
      " 'Dictyocha' 'Dinobryon' 'Dinophysis' 'Ditylum' 'Ephemera' 'Eucampia'\n",
      " 'Euglena' 'Guinardia' 'Guinardia_flaccida' 'Guinardia_striata'\n",
      " 'Gyrodinium' 'Laboea' 'Lauderia' 'Leptocylindrus' 'Licmophora'\n",
      " 'Myrionecta' 'Paralia' 'Phaeocystis' 'Pleurosigma' 'Prorocentrum'\n",
      " 'Pseudonitzschia' 'Pyramimonas' 'Rhizosolenia' 'Skeletonema'\n",
      " 'Thalassionema' 'Thalassiosira' 'Thalassiosira_dirty' 'bad' 'ciliate_mix'\n",
      " 'clusterflagellate' 'detritus' 'dino30' 'kiteflagellates' 'mix'\n",
      " 'mix_elongated' 'na' 'pennate' 'tintinnid']\n",
      "col_0   count\n",
      "year         \n",
      "2006   131002\n",
      "2007   273080\n",
      "2008   427308\n",
      "2009   732398\n",
      "2010   327996\n",
      "2011   419692\n",
      "2012   394766\n",
      "2013   422255\n",
      "2014   329322\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuración del entrenamiento\n",
    "Configuramos el entrenamiento y los archivos de salida. Es importante ver que aquí podemos configurar cuantos ejemplos usamos para entrenar. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import torch,torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print(\"Using pytorch {}\".format(torch.__version__))\n",
    "\n",
    "torch.manual_seed(0) #Reproducible\n",
    "random.seed(0) #it seems that the transforms uses this random\n",
    "np.random.seed(0)\n",
    "\n",
    "gpus = [0,1] #gpus to use\n",
    "num_gpus = len(gpus)\n",
    "\n",
    "num_workers = 6*num_gpus\n",
    "batch_size = 512 #512 for resnet 18, and 34 (with 2 gpus). 256 for resnet 50\n",
    "batch_size_val = 512\n",
    "\n",
    "print(\"Num workers {}. Batch size training {}. Batch size validation {}\".format(num_workers,batch_size,batch_size_val))\n",
    "\n",
    "num_epochs = 50 # @param\n",
    "\n",
    "#Subsample data\n",
    "train_size = 0.1\n",
    "\n",
    "model_save_path_fe=\"model18_fe_0_1.pt\" #Where to save the model once trained\n",
    "model_save_path_ft=\"model18_ft_0_1.pt\" #Where to save the model once trained\n",
    "\n",
    "hdf5_files_path = '/media/nas/pgonzalez/IFCB_HDF5/output/' #Directory with the dataset\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using %s\"%device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using pytorch 1.7.1\n",
      "Num workers 12. Batch size training 512. Batch size validation 512\n",
      "Using cuda:0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import torchvision.transforms as T\n",
    "from h5ifcbdataset import H5IFCBDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Define transofrmations\n",
    "train_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.RandomResizedCrop(size=224),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "  T.Resize(size=256),\n",
    "  T.CenterCrop(size=224),\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "#files to load\n",
    "files = [hdf5_files_path+s+'.hdf5' for s in samplestraining]\n",
    "#Define data loader\n",
    "\n",
    "train_dset = H5IFCBDataset(files,classes,classattribute=\"AutoClass\",verbose=1,trainingset=True,transform=train_transform)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading samples: 100%|██████████| 164/164 [02:21<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "indexes_train,_ = train_test_split(list(range(len(train_dset))),train_size=train_size,stratify=train_dset.targets,random_state=0)\n",
    "train_subset = torch.utils.data.Subset(train_dset, indexes_train)\n",
    "train_loader = DataLoader(train_subset,batch_size=batch_size,num_workers=num_workers,shuffle=True,pin_memory=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def load_network(model_save_path):\n",
    "    base_model = torchvision.models.resnet18(pretrained=True) #From which model to start\n",
    "    \n",
    "    model = base_model\n",
    "    print(\"Adjusting the CNN for %s classes\" % len(classes))\n",
    "    model.fc = nn.Linear(model.fc.in_features, len(classes))\n",
    "    \n",
    "    print(model)\n",
    "\n",
    "    print(\"Let's use\", len(gpus), \"GPUs!\")\n",
    "    model = nn.DataParallel(model,device_ids=gpus)\n",
    "\n",
    "    #Define loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    if os.path.isfile(model_save_path):\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        is_trained=True\n",
    "    else:\n",
    "        is_trained=False\n",
    "    \n",
    "    model = model.to(device) #Send model to gpu\n",
    "    return model,loss_fn,is_trained"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import time\n",
    "import torch.nn.functional as nnf\n",
    "\n",
    "\n",
    "def run_epoch(model, loss_fn, loader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    loss_epoch = 0 \n",
    "    start_time = time.time()\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    for step, (x, y, _) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Run the model forward to compute scores and loss.\n",
    "        scores = model(x)\n",
    "        loss = loss_fn(scores, y)\n",
    "        loss_epoch = loss_epoch + loss.item()\n",
    "        # Run the model backward and take a step using the optimizer.\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 50== 0:\n",
    "            spent = time.time()-start_time\n",
    "            print(f\"Step [{step}/{len(loader)}]\\t Loss: {loss.item()} \\t Time: {spent} secs [{(batch_size*50)/spent} ej/sec]]\")\n",
    "            start_time = time.time()\n",
    "\n",
    "    return loss_epoch\n",
    "\n",
    "def make_preds(model, loader, device):\n",
    "    \"\"\"\n",
    "    Check the accuracy of the model.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # Set the model to eval mode\n",
    "        model.eval()\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        y_probs = []\n",
    "        sample = []\n",
    "        for x, y, s in loader: #The idea is that the dataloader can give me the sample of the image so we can return it\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            # Run the model forward, and compare the argmax score with the ground-truth\n",
    "            # category.\n",
    "            output = model(x)\n",
    "            predicted = output.argmax(1)\n",
    "            prob = nnf.softmax(output, dim=1)\n",
    "            y_probs.extend(prob.cpu().detach().numpy())\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            sample.extend(s)\n",
    "    return y_true,y_pred,y_probs,sample"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Proceso de entrenamiento\n",
    "\n",
    "El parámetro importante aquí es `only_fe_layer`. En caso de que sea true solo toca los pesos de la última capa. Si es false, se reentrena toda la red."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def finetune(model,loss_fn,train_loader,only_fe_layer,model_save_path,device):\n",
    "    if only_fe_layer:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.module.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer = torch.optim.Adam(model.module.fc.parameters(), lr=1e-4)\n",
    "    else:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Run an epoch over the training data.\n",
    "        print('Starting epoch %d / %d' % (epoch + 1,num_epochs))\n",
    "        loss_epoch = run_epoch(model, loss_fn, train_loader, optimizer, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]\\t Loss: {loss_epoch / len(train_loader)}\")\n",
    "        \n",
    "    #Save model in this point\n",
    "    #TODO\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(\"Fine tune done and model saved.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimento\n",
    "\n",
    "Primero entrenamos la red como FE y luego como FT y mostramos los resultados."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import threading\n",
    "\n",
    "print('Starting process...')\n",
    "print('Training model FE')\n",
    "\n",
    "model_fe,loss_fn,is_trained = load_network(model_save_path_fe)\n",
    "\n",
    "if not is_trained:\n",
    "    finetune(model_fe,loss_fn,train_loader,True,model_save_path_fe,device)\n",
    "else:\n",
    "    print(\"Model FE was trained already\")\n",
    "    \n",
    "print('Training model FT')\n",
    "model_ft,loss_fn,is_trained = load_network(model_save_path_ft)\n",
    "\n",
    "if not is_trained:\n",
    "    finetune(model_ft,loss_fn,train_loader,False,model_save_path_ft,device)\n",
    "else:\n",
    "    print(\"Model FTwas trained already\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting process...\n",
      "Training model FE\n",
      "Adjusting the CNN for 47 classes\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=47, bias=True)\n",
      ")\n",
      "Let's use 2 GPUs!\n",
      "Starting epoch 1 / 50\n",
      "Step [0/79]\t Loss: 3.6322360038757324 \t Time: 7.040963649749756 secs [3635.8659515178515 ej/sec]]\n",
      "Step [50/79]\t Loss: 1.4289036989212036 \t Time: 10.324557304382324 secs [2479.5251985413374 ej/sec]]\n",
      "Epoch [1/50]\t Loss: 2.0121665212172495\n",
      "Starting epoch 2 / 50\n",
      "Step [0/79]\t Loss: 1.2607645988464355 \t Time: 3.803295850753784 secs [6731.004109219186 ej/sec]]\n",
      "Step [50/79]\t Loss: 1.1824350357055664 \t Time: 10.613616228103638 secs [2411.9960105787636 ej/sec]]\n",
      "Epoch [2/50]\t Loss: 1.248920920528943\n",
      "Starting epoch 3 / 50\n",
      "Step [0/79]\t Loss: 1.17328941822052 \t Time: 4.1592419147491455 secs [6154.967786129363 ej/sec]]\n",
      "Step [50/79]\t Loss: 1.0731124877929688 \t Time: 10.521845579147339 secs [2433.033236178187 ej/sec]]\n",
      "Epoch [3/50]\t Loss: 1.1496682529208027\n",
      "Starting epoch 4 / 50\n",
      "Step [0/79]\t Loss: 1.078924536705017 \t Time: 3.810328722000122 secs [6718.58043433114 ej/sec]]\n",
      "Step [50/79]\t Loss: 1.0487923622131348 \t Time: 10.642099857330322 secs [2405.5402921601617 ej/sec]]\n",
      "Epoch [4/50]\t Loss: 1.0731526967845386\n",
      "Starting epoch 5 / 50\n",
      "Step [0/79]\t Loss: 1.0357561111450195 \t Time: 2.980907440185547 secs [8587.988897235442 ej/sec]]\n",
      "Step [50/79]\t Loss: 1.0325210094451904 \t Time: 10.973332643508911 secs [2332.9284577136414 ej/sec]]\n",
      "Epoch [5/50]\t Loss: 1.016801339161547\n",
      "Starting epoch 6 / 50\n",
      "Step [0/79]\t Loss: 0.8737892508506775 \t Time: 3.008157730102539 secs [8510.192050044987 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.8542983531951904 \t Time: 11.308173179626465 secs [2263.849305573301 ej/sec]]\n",
      "Epoch [6/50]\t Loss: 0.9719059588033941\n",
      "Starting epoch 7 / 50\n",
      "Step [0/79]\t Loss: 0.9933017492294312 \t Time: 3.5747785568237305 secs [7161.282746069218 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.8178701400756836 \t Time: 10.70322585105896 secs [2391.802280568262 ej/sec]]\n",
      "Epoch [7/50]\t Loss: 0.9363894304142723\n",
      "Starting epoch 8 / 50\n",
      "Step [0/79]\t Loss: 0.9747121334075928 \t Time: 3.3586950302124023 secs [7622.007883931357 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.9290462732315063 \t Time: 10.993234872817993 secs [2328.7049077154597 ej/sec]]\n",
      "Epoch [8/50]\t Loss: 0.9027525744860685\n",
      "Starting epoch 9 / 50\n",
      "Step [0/79]\t Loss: 0.9153944253921509 \t Time: 3.781620740890503 secs [6769.584195260063 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7930032014846802 \t Time: 10.575427770614624 secs [2420.705862237871 ej/sec]]\n",
      "Epoch [9/50]\t Loss: 0.8776956758921659\n",
      "Starting epoch 10 / 50\n",
      "Step [0/79]\t Loss: 0.84568852186203 \t Time: 3.2479233741760254 secs [7881.959347792352 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.8841270804405212 \t Time: 10.947293519973755 secs [2338.477538150578 ej/sec]]\n",
      "Epoch [10/50]\t Loss: 0.8588224249550059\n",
      "Starting epoch 11 / 50\n",
      "Step [0/79]\t Loss: 0.9140188694000244 \t Time: 4.059622526168823 secs [6306.005012776254 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7770459055900574 \t Time: 10.71467924118042 secs [2389.2455783099754 ej/sec]]\n",
      "Epoch [11/50]\t Loss: 0.8409470538549786\n",
      "Starting epoch 12 / 50\n",
      "Step [0/79]\t Loss: 0.8699188828468323 \t Time: 3.0377020835876465 secs [8427.42286622307 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.779575526714325 \t Time: 11.292348861694336 secs [2267.0217076661324 ej/sec]]\n",
      "Epoch [12/50]\t Loss: 0.8253649737261519\n",
      "Starting epoch 13 / 50\n",
      "Step [0/79]\t Loss: 0.8420616984367371 \t Time: 3.4721269607543945 secs [7373.002280549628 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7456441521644592 \t Time: 11.096229314804077 secs [2307.090027947211 ej/sec]]\n",
      "Epoch [13/50]\t Loss: 0.8096105992039547\n",
      "Starting epoch 14 / 50\n",
      "Step [0/79]\t Loss: 0.7288314700126648 \t Time: 4.046050548553467 secs [6327.1577289493935 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.9530681371688843 \t Time: 10.507928133010864 secs [2436.2557181540947 ej/sec]]\n",
      "Epoch [14/50]\t Loss: 0.796569101418121\n",
      "Starting epoch 15 / 50\n",
      "Step [0/79]\t Loss: 0.8518587946891785 \t Time: 4.0630223751068115 secs [6300.728284649677 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7497791647911072 \t Time: 10.680001258850098 secs [2397.003462783891 ej/sec]]\n",
      "Epoch [15/50]\t Loss: 0.7878945537760288\n",
      "Starting epoch 16 / 50\n",
      "Step [0/79]\t Loss: 0.7940127849578857 \t Time: 3.7371068000793457 secs [6850.218998144893 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7821614146232605 \t Time: 10.774029016494751 secs [2376.08418919302 ej/sec]]\n",
      "Epoch [16/50]\t Loss: 0.7725875822803642\n",
      "Starting epoch 17 / 50\n",
      "Step [0/79]\t Loss: 0.7755135297775269 \t Time: 4.069362640380859 secs [6290.911442978218 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6972920298576355 \t Time: 10.446118354797363 secs [2450.671065606225 ej/sec]]\n",
      "Epoch [17/50]\t Loss: 0.762503795231445\n",
      "Starting epoch 18 / 50\n",
      "Step [0/79]\t Loss: 0.7034413814544678 \t Time: 3.2690765857696533 secs [7830.957558913499 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7250452041625977 \t Time: 11.349167823791504 secs [2255.671992649027 ej/sec]]\n",
      "Epoch [18/50]\t Loss: 0.7528844747362258\n",
      "Starting epoch 19 / 50\n",
      "Step [0/79]\t Loss: 0.6970633268356323 \t Time: 3.890484571456909 secs [6580.157183456792 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6195076107978821 \t Time: 10.64670991897583 secs [2404.4986850231207 ej/sec]]\n",
      "Epoch [19/50]\t Loss: 0.7480775027335445\n",
      "Starting epoch 20 / 50\n",
      "Step [0/79]\t Loss: 0.7244731783866882 \t Time: 4.115965127944946 secs [6219.683404553474 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6779912710189819 \t Time: 10.559062004089355 secs [2424.45777760236 ej/sec]]\n",
      "Epoch [20/50]\t Loss: 0.7378786228880098\n",
      "Starting epoch 21 / 50\n",
      "Step [0/79]\t Loss: 0.7265906929969788 \t Time: 4.040828704833984 secs [6335.334128213624 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.8554912209510803 \t Time: 10.54758358001709 secs [2427.0961975120485 ej/sec]]\n",
      "Epoch [21/50]\t Loss: 0.7345627802836744\n",
      "Starting epoch 22 / 50\n",
      "Step [0/79]\t Loss: 0.8269663453102112 \t Time: 3.262558937072754 secs [7846.601546137565 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6644860506057739 \t Time: 10.871628046035767 secs [2354.753114399898 ej/sec]]\n",
      "Epoch [22/50]\t Loss: 0.7298728997194315\n",
      "Starting epoch 23 / 50\n",
      "Step [0/79]\t Loss: 0.7659958600997925 \t Time: 3.5557477474212646 secs [7199.610832507984 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7634851336479187 \t Time: 10.741426944732666 secs [2383.2960119468685 ej/sec]]\n",
      "Epoch [23/50]\t Loss: 0.718088275269617\n",
      "Starting epoch 24 / 50\n",
      "Step [0/79]\t Loss: 0.8182405829429626 \t Time: 2.9157629013061523 secs [8779.863406771572 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7532783150672913 \t Time: 11.725457429885864 secs [2183.2836930310864 ej/sec]]\n",
      "Epoch [24/50]\t Loss: 0.7170024106774149\n",
      "Starting epoch 25 / 50\n",
      "Step [0/79]\t Loss: 0.7004354596138 \t Time: 3.862240791320801 secs [6628.276532506242 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6405037641525269 \t Time: 10.642261266708374 secs [2405.5038077370955 ej/sec]]\n",
      "Epoch [25/50]\t Loss: 0.7144699473924274\n",
      "Starting epoch 26 / 50\n",
      "Step [0/79]\t Loss: 0.644751250743866 \t Time: 3.3463571071624756 secs [7650.1100092414745 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7017935514450073 \t Time: 10.98824405670166 secs [2329.7625960889286 ej/sec]]\n",
      "Epoch [26/50]\t Loss: 0.7105555541907684\n",
      "Starting epoch 27 / 50\n",
      "Step [0/79]\t Loss: 0.7546166181564331 \t Time: 3.65346622467041 secs [7007.044386268947 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6650791168212891 \t Time: 10.688122272491455 secs [2395.182179557206 ej/sec]]\n",
      "Epoch [27/50]\t Loss: 0.7062241510499881\n",
      "Starting epoch 28 / 50\n",
      "Step [0/79]\t Loss: 0.6458202600479126 \t Time: 2.918208122253418 secs [8772.506595667986 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6456573605537415 \t Time: 11.221820831298828 secs [2281.2697141446897 ej/sec]]\n",
      "Epoch [28/50]\t Loss: 0.6997000197821026\n",
      "Starting epoch 29 / 50\n",
      "Step [0/79]\t Loss: 0.6837838888168335 \t Time: 3.1321821212768555 secs [8173.2156716238405 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6451999545097351 \t Time: 10.915140628814697 secs [2345.3660260151837 ej/sec]]\n",
      "Epoch [29/50]\t Loss: 0.6976926960522616\n",
      "Starting epoch 30 / 50\n",
      "Step [0/79]\t Loss: 0.6595731973648071 \t Time: 2.936450719833374 secs [8718.007704707077 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6799899935722351 \t Time: 11.585055828094482 secs [2209.743343482075 ej/sec]]\n",
      "Epoch [30/50]\t Loss: 0.6925196625009368\n",
      "Starting epoch 31 / 50\n",
      "Step [0/79]\t Loss: 0.6941291689872742 \t Time: 3.6261463165283203 secs [7059.83646697122 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6889088749885559 \t Time: 10.597253799438477 secs [2415.720193599259 ej/sec]]\n",
      "Epoch [31/50]\t Loss: 0.6897744498675382\n",
      "Starting epoch 32 / 50\n",
      "Step [0/79]\t Loss: 0.661857545375824 \t Time: 4.094379186630249 secs [6252.474143966446 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6618829369544983 \t Time: 10.691999912261963 secs [2394.313525072238 ej/sec]]\n",
      "Epoch [32/50]\t Loss: 0.6872589882416061\n",
      "Starting epoch 33 / 50\n",
      "Step [0/79]\t Loss: 0.7132487893104553 \t Time: 3.8550381660461426 secs [6640.660584239098 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6430183053016663 \t Time: 10.668408155441284 secs [2399.608228988038 ej/sec]]\n",
      "Epoch [33/50]\t Loss: 0.6827370863926562\n",
      "Starting epoch 34 / 50\n",
      "Step [0/79]\t Loss: 0.6383315920829773 \t Time: 2.7868337631225586 secs [9186.052049016376 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.669146716594696 \t Time: 11.737053632736206 secs [2181.126609884289 ej/sec]]\n",
      "Epoch [34/50]\t Loss: 0.6809348305569419\n",
      "Starting epoch 35 / 50\n",
      "Step [0/79]\t Loss: 0.7101700305938721 \t Time: 3.36275577545166 secs [7612.803816108709 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.688732922077179 \t Time: 11.022687435150146 secs [2322.4826205598824 ej/sec]]\n",
      "Epoch [35/50]\t Loss: 0.6763764453839652\n",
      "Starting epoch 36 / 50\n",
      "Step [0/79]\t Loss: 0.6710429191589355 \t Time: 3.507777452468872 secs [7298.068462690529 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.5828137993812561 \t Time: 10.957406997680664 secs [2336.3191679763936 ej/sec]]\n",
      "Epoch [36/50]\t Loss: 0.6759271810326395\n",
      "Starting epoch 37 / 50\n",
      "Step [0/79]\t Loss: 0.636349081993103 \t Time: 3.2086479663848877 secs [7978.438354159167 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7597404718399048 \t Time: 10.596307277679443 secs [2415.9359793128156 ej/sec]]\n",
      "Epoch [37/50]\t Loss: 0.6743551394607448\n",
      "Starting epoch 38 / 50\n",
      "Step [0/79]\t Loss: 0.6069875359535217 \t Time: 3.1391797065734863 secs [8154.996652913256 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6667920351028442 \t Time: 10.889490365982056 secs [2350.890550394577 ej/sec]]\n",
      "Epoch [38/50]\t Loss: 0.6673710074605821\n",
      "Starting epoch 39 / 50\n",
      "Step [0/79]\t Loss: 0.7251190543174744 \t Time: 4.074231147766113 secs [6283.394110821717 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6473070383071899 \t Time: 10.601320505142212 secs [2414.7935144100793 ej/sec]]\n",
      "Epoch [39/50]\t Loss: 0.6673960617825955\n",
      "Starting epoch 40 / 50\n",
      "Step [0/79]\t Loss: 0.679711103439331 \t Time: 4.211720943450928 secs [6078.275446954069 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6798475980758667 \t Time: 10.602717876434326 secs [2414.4752598669756 ej/sec]]\n",
      "Epoch [40/50]\t Loss: 0.6615231255941754\n",
      "Starting epoch 41 / 50\n",
      "Step [0/79]\t Loss: 0.6052632331848145 \t Time: 4.061895132064819 secs [6302.476840899269 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6959953904151917 \t Time: 10.580871343612671 secs [2419.46047434495 ej/sec]]\n",
      "Epoch [41/50]\t Loss: 0.6667588836030115\n",
      "Starting epoch 42 / 50\n",
      "Step [0/79]\t Loss: 0.6545404195785522 \t Time: 3.921856164932251 secs [6527.521388699943 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6711750626564026 \t Time: 10.562549114227295 secs [2423.6573693672026 ej/sec]]\n",
      "Epoch [42/50]\t Loss: 0.6604194859915142\n",
      "Starting epoch 43 / 50\n",
      "Step [0/79]\t Loss: 0.6113157868385315 \t Time: 3.4921622276306152 secs [7330.701820622249 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6401509046554565 \t Time: 10.862340450286865 secs [2356.766492190357 ej/sec]]\n",
      "Epoch [43/50]\t Loss: 0.6590368151664734\n",
      "Starting epoch 44 / 50\n",
      "Step [0/79]\t Loss: 0.6382714509963989 \t Time: 2.8129804134368896 secs [9100.667703804595 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.5531478524208069 \t Time: 11.14345932006836 secs [2297.3117471606615 ej/sec]]\n",
      "Epoch [44/50]\t Loss: 0.6547127169898793\n",
      "Starting epoch 45 / 50\n",
      "Step [0/79]\t Loss: 0.5772203207015991 \t Time: 3.0297353267669678 secs [8449.582963181729 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.5971165299415588 \t Time: 11.077653646469116 secs [2310.9586936904934 ej/sec]]\n",
      "Epoch [45/50]\t Loss: 0.6588338439977621\n",
      "Starting epoch 46 / 50\n",
      "Step [0/79]\t Loss: 0.5837321877479553 \t Time: 4.158318996429443 secs [6156.333850765547 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6274330019950867 \t Time: 10.681961297988892 secs [2396.5636352586066 ej/sec]]\n",
      "Epoch [46/50]\t Loss: 0.6558395088473453\n",
      "Starting epoch 47 / 50\n",
      "Step [0/79]\t Loss: 0.627627968788147 \t Time: 4.030659914016724 secs [6351.317289502729 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7961880564689636 \t Time: 10.654384851455688 secs [2402.766593934545 ej/sec]]\n",
      "Epoch [47/50]\t Loss: 0.6495530363879626\n",
      "Starting epoch 48 / 50\n",
      "Step [0/79]\t Loss: 0.656046450138092 \t Time: 4.203736066818237 secs [6089.820957616962 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6358595490455627 \t Time: 10.64717698097229 secs [2404.393206363536 ej/sec]]\n",
      "Epoch [48/50]\t Loss: 0.6465819078155711\n",
      "Starting epoch 49 / 50\n",
      "Step [0/79]\t Loss: 0.7159237265586853 \t Time: 2.801384449005127 secs [9138.338727157383 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6525466442108154 \t Time: 11.66387152671814 secs [2194.8115547533866 ej/sec]]\n",
      "Epoch [49/50]\t Loss: 0.6463531429254556\n",
      "Starting epoch 50 / 50\n",
      "Step [0/79]\t Loss: 0.6487938761711121 \t Time: 4.029833555221558 secs [6352.6196924012975 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.7911096811294556 \t Time: 10.601311922073364 secs [2414.795469483106 ej/sec]]\n",
      "Epoch [50/50]\t Loss: 0.6484609050086781\n",
      "Fine tune done and model saved.\n",
      "Training model FT\n",
      "Adjusting the CNN for 47 classes\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=47, bias=True)\n",
      ")\n",
      "Let's use 2 GPUs!\n",
      "Starting epoch 1 / 50\n",
      "Step [0/79]\t Loss: 4.125272750854492 \t Time: 3.4364659786224365 secs [7449.513587287769 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.6926032900810242 \t Time: 22.734354257583618 secs [1126.049137351701 ej/sec]]\n",
      "Epoch [1/50]\t Loss: 1.037140333954292\n",
      "Starting epoch 2 / 50\n",
      "Step [0/79]\t Loss: 0.6459536552429199 \t Time: 4.035793304443359 secs [6343.238632120904 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.5369963049888611 \t Time: 22.766742944717407 secs [1124.4471843057374 ej/sec]]\n",
      "Epoch [2/50]\t Loss: 0.5837395870232884\n",
      "Starting epoch 3 / 50\n",
      "Step [0/79]\t Loss: 0.5657885670661926 \t Time: 4.109819173812866 secs [6228.9845166715 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.5495046377182007 \t Time: 22.76827335357666 secs [1124.3716026441023 ej/sec]]\n",
      "Epoch [3/50]\t Loss: 0.5176051153412348\n",
      "Starting epoch 4 / 50\n",
      "Step [0/79]\t Loss: 0.5732232332229614 \t Time: 3.999302387237549 secs [6401.116375119304 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.49131524562835693 \t Time: 22.787208080291748 secs [1123.4373210529895 ej/sec]]\n",
      "Epoch [4/50]\t Loss: 0.4871131377884104\n",
      "Starting epoch 5 / 50\n",
      "Step [0/79]\t Loss: 0.4843154847621918 \t Time: 3.1078317165374756 secs [8237.254245066299 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.46156513690948486 \t Time: 22.862587690353394 secs [1119.733266711608 ej/sec]]\n",
      "Epoch [5/50]\t Loss: 0.4626465024827402\n",
      "Starting epoch 6 / 50\n",
      "Step [0/79]\t Loss: 0.417474627494812 \t Time: 4.212453842163086 secs [6077.217925515465 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.4636947512626648 \t Time: 22.719773054122925 secs [1126.7718184955374 ej/sec]]\n",
      "Epoch [6/50]\t Loss: 0.44397992272920245\n",
      "Starting epoch 7 / 50\n",
      "Step [0/79]\t Loss: 0.4179654121398926 \t Time: 4.032339096069336 secs [6348.672418188862 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3630962073802948 \t Time: 22.771862030029297 secs [1124.1944100241444 ej/sec]]\n",
      "Epoch [7/50]\t Loss: 0.42830444287650193\n",
      "Starting epoch 8 / 50\n",
      "Step [0/79]\t Loss: 0.3427172601222992 \t Time: 4.035680055618286 secs [6343.416635409655 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3694709837436676 \t Time: 22.66095733642578 secs [1129.696315117717 ej/sec]]\n",
      "Epoch [8/50]\t Loss: 0.42193933491465413\n",
      "Starting epoch 9 / 50\n",
      "Step [0/79]\t Loss: 0.41255834698677063 \t Time: 3.8155174255371094 secs [6709.443869568043 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.39359423518180847 \t Time: 22.89023208618164 secs [1118.3809715697112 ej/sec]]\n",
      "Epoch [9/50]\t Loss: 0.4107174514969693\n",
      "Starting epoch 10 / 50\n",
      "Step [0/79]\t Loss: 0.4348820149898529 \t Time: 4.234188556671143 secs [6046.022669365095 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.4293157756328583 \t Time: 22.66564917564392 secs [1129.462465496434 ej/sec]]\n",
      "Epoch [10/50]\t Loss: 0.39965481358238414\n",
      "Starting epoch 11 / 50\n",
      "Step [0/79]\t Loss: 0.3755927085876465 \t Time: 3.0198974609375 secs [8477.10901814948 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3710092604160309 \t Time: 23.155617713928223 secs [1105.5632510551195 ej/sec]]\n",
      "Epoch [11/50]\t Loss: 0.3901944224593006\n",
      "Starting epoch 12 / 50\n",
      "Step [0/79]\t Loss: 0.37782996892929077 \t Time: 3.6780455112457275 secs [6960.218388197558 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.32227903604507446 \t Time: 22.603360891342163 secs [1132.5749353409497 ej/sec]]\n",
      "Epoch [12/50]\t Loss: 0.3877075254162656\n",
      "Starting epoch 13 / 50\n",
      "Step [0/79]\t Loss: 0.3154474198818207 \t Time: 3.3157355785369873 secs [7720.760414585161 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3905737102031708 \t Time: 22.768115997314453 secs [1124.379373463293 ej/sec]]\n",
      "Epoch [13/50]\t Loss: 0.37853515940376475\n",
      "Starting epoch 14 / 50\n",
      "Step [0/79]\t Loss: 0.39749959111213684 \t Time: 2.879465341567993 secs [8890.539375639679 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3823636770248413 \t Time: 23.035670280456543 secs [1111.3199524182735 ej/sec]]\n",
      "Epoch [14/50]\t Loss: 0.3736059982565385\n",
      "Starting epoch 15 / 50\n",
      "Step [0/79]\t Loss: 0.38191723823547363 \t Time: 3.1042721271514893 secs [8246.699693654375 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.407710462808609 \t Time: 23.118512392044067 secs [1107.3376853092807 ej/sec]]\n",
      "Epoch [15/50]\t Loss: 0.36571027319642563\n",
      "Starting epoch 16 / 50\n",
      "Step [0/79]\t Loss: 0.3519207835197449 \t Time: 3.6378188133239746 secs [7037.183904332107 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.4132813513278961 \t Time: 22.862987995147705 secs [1119.7136614616245 ej/sec]]\n",
      "Epoch [16/50]\t Loss: 0.3617014262495162\n",
      "Starting epoch 17 / 50\n",
      "Step [0/79]\t Loss: 0.36842072010040283 \t Time: 4.088406801223755 secs [6261.607820517598 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3726472556591034 \t Time: 22.737699508666992 secs [1125.883469004504 ej/sec]]\n",
      "Epoch [17/50]\t Loss: 0.35330171864244003\n",
      "Starting epoch 18 / 50\n",
      "Step [0/79]\t Loss: 0.35081949830055237 \t Time: 4.1116249561309814 secs [6226.248812364801 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.42524847388267517 \t Time: 22.780182600021362 secs [1123.7837926714421 ej/sec]]\n",
      "Epoch [18/50]\t Loss: 0.3462465216841879\n",
      "Starting epoch 19 / 50\n",
      "Step [0/79]\t Loss: 0.3653397858142853 \t Time: 2.970247983932495 secs [8618.808981096108 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.4232986867427826 \t Time: 22.87223172187805 secs [1119.261133381783 ej/sec]]\n",
      "Epoch [19/50]\t Loss: 0.3471585802639587\n",
      "Starting epoch 20 / 50\n",
      "Step [0/79]\t Loss: 0.2799079716205597 \t Time: 3.9977712631225586 secs [6403.567967018825 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.371919184923172 \t Time: 22.819136142730713 secs [1121.8654308329355 ej/sec]]\n",
      "Epoch [20/50]\t Loss: 0.3456377236148979\n",
      "Starting epoch 21 / 50\n",
      "Step [0/79]\t Loss: 0.3822294771671295 \t Time: 3.0819246768951416 secs [8306.497622060802 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3356610834598541 \t Time: 23.504955530166626 secs [1089.132032908744 ej/sec]]\n",
      "Epoch [21/50]\t Loss: 0.3399592647446862\n",
      "Starting epoch 22 / 50\n",
      "Step [0/79]\t Loss: 0.3481394350528717 \t Time: 3.599870204925537 secs [7111.367505687482 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3356608748435974 \t Time: 22.717178106307983 secs [1126.9005278825334 ej/sec]]\n",
      "Epoch [22/50]\t Loss: 0.33402958399132837\n",
      "Starting epoch 23 / 50\n",
      "Step [0/79]\t Loss: 0.38421547412872314 \t Time: 4.008327484130859 secs [6386.703706558783 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3395007252693176 \t Time: 22.67229175567627 secs [1129.1315529931262 ej/sec]]\n",
      "Epoch [23/50]\t Loss: 0.333017333776136\n",
      "Starting epoch 24 / 50\n",
      "Step [0/79]\t Loss: 0.2824787497520447 \t Time: 3.1936635971069336 secs [8015.872436655649 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.2468344122171402 \t Time: 22.931624174118042 secs [1116.3622692235485 ej/sec]]\n",
      "Epoch [24/50]\t Loss: 0.32756925138491616\n",
      "Starting epoch 25 / 50\n",
      "Step [0/79]\t Loss: 0.359326034784317 \t Time: 4.290242433547974 secs [5967.028762714731 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.2758243680000305 \t Time: 22.749409437179565 secs [1125.3039368205175 ej/sec]]\n",
      "Epoch [25/50]\t Loss: 0.32751708449442174\n",
      "Starting epoch 26 / 50\n",
      "Step [0/79]\t Loss: 0.2837049067020416 \t Time: 2.900753974914551 secs [8825.29170739277 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3019520938396454 \t Time: 22.81527543067932 secs [1122.0552685318935 ej/sec]]\n",
      "Epoch [26/50]\t Loss: 0.32283919861045063\n",
      "Starting epoch 27 / 50\n",
      "Step [0/79]\t Loss: 0.2921263575553894 \t Time: 3.9805400371551514 secs [6431.288157145642 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.31761255860328674 \t Time: 22.861448049545288 secs [1119.7890852985222 ej/sec]]\n",
      "Epoch [27/50]\t Loss: 0.32108839101429226\n",
      "Starting epoch 28 / 50\n",
      "Step [0/79]\t Loss: 0.4196460247039795 \t Time: 2.8401899337768555 secs [9013.48170259775 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.2977787256240845 \t Time: 23.223896265029907 secs [1102.312881002142 ej/sec]]\n",
      "Epoch [28/50]\t Loss: 0.31556866308556325\n",
      "Starting epoch 29 / 50\n",
      "Step [0/79]\t Loss: 0.3280482590198517 \t Time: 4.104108810424805 secs [6237.6513836508675 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.33742067217826843 \t Time: 22.6928813457489 secs [1128.1070750760214 ej/sec]]\n",
      "Epoch [29/50]\t Loss: 0.3163752936864201\n",
      "Starting epoch 30 / 50\n",
      "Step [0/79]\t Loss: 0.2917729318141937 \t Time: 2.858750343322754 secs [8954.961757956404 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3208049237728119 \t Time: 22.78981328010559 secs [1123.3088961877352 ej/sec]]\n",
      "Epoch [30/50]\t Loss: 0.3130224671544908\n",
      "Starting epoch 31 / 50\n",
      "Step [0/79]\t Loss: 0.30051347613334656 \t Time: 3.200317859649658 secs [7999.205429801418 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.28966856002807617 \t Time: 22.82943844795227 secs [1121.3591634487286 ej/sec]]\n",
      "Epoch [31/50]\t Loss: 0.3087729721129695\n",
      "Starting epoch 32 / 50\n",
      "Step [0/79]\t Loss: 0.3074321746826172 \t Time: 3.9863970279693604 secs [6421.839024157722 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.25821173191070557 \t Time: 22.787551164627075 secs [1123.420406828912 ej/sec]]\n",
      "Epoch [32/50]\t Loss: 0.3047512988500957\n",
      "Starting epoch 33 / 50\n",
      "Step [0/79]\t Loss: 0.2825920283794403 \t Time: 4.152576208114624 secs [6164.8477275322675 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.24620790779590607 \t Time: 22.638505458831787 secs [1130.81669841473 ej/sec]]\n",
      "Epoch [33/50]\t Loss: 0.30362859330599823\n",
      "Starting epoch 34 / 50\n",
      "Step [0/79]\t Loss: 0.29645460844039917 \t Time: 3.9964237213134766 secs [6405.727166384206 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.32093262672424316 \t Time: 22.776703357696533 secs [1123.9554556234514 ej/sec]]\n",
      "Epoch [34/50]\t Loss: 0.3004316263183763\n",
      "Starting epoch 35 / 50\n",
      "Step [0/79]\t Loss: 0.31762799620628357 \t Time: 2.8935251235961914 secs [8847.339804046102 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.2673158049583435 \t Time: 22.720060110092163 secs [1126.7575823282518 ej/sec]]\n",
      "Epoch [35/50]\t Loss: 0.29340778781643395\n",
      "Starting epoch 36 / 50\n",
      "Step [0/79]\t Loss: 0.3299211263656616 \t Time: 4.008887767791748 secs [6385.811098448755 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.2510281503200531 \t Time: 22.673956632614136 secs [1129.0486444336343 ej/sec]]\n",
      "Epoch [36/50]\t Loss: 0.2902813980096503\n",
      "Starting epoch 37 / 50\n",
      "Step [0/79]\t Loss: 0.2638951539993286 \t Time: 3.88631010055542 secs [6587.225243899431 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.2714337110519409 \t Time: 22.770782232284546 secs [1124.2477196810644 ej/sec]]\n",
      "Epoch [37/50]\t Loss: 0.287261224057101\n",
      "Starting epoch 38 / 50\n",
      "Step [0/79]\t Loss: 0.3048825263977051 \t Time: 3.334702253341675 secs [7676.847303037767 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.23354841768741608 \t Time: 22.979589223861694 secs [1114.032098250795 ej/sec]]\n",
      "Epoch [38/50]\t Loss: 0.29063880632195294\n",
      "Starting epoch 39 / 50\n",
      "Step [0/79]\t Loss: 0.2856295704841614 \t Time: 2.9983866214752197 secs [8537.924968263327 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.26580846309661865 \t Time: 22.994855165481567 secs [1113.292508944745 ej/sec]]\n",
      "Epoch [39/50]\t Loss: 0.28700806162779846\n",
      "Starting epoch 40 / 50\n",
      "Step [0/79]\t Loss: 0.28581392765045166 \t Time: 3.2105228900909424 secs [7973.778999991757 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3110295534133911 \t Time: 22.88439702987671 secs [1118.6661359955403 ej/sec]]\n",
      "Epoch [40/50]\t Loss: 0.28460716258121443\n",
      "Starting epoch 41 / 50\n",
      "Step [0/79]\t Loss: 0.2983509302139282 \t Time: 4.296501874923706 secs [5958.335582119253 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.2899305820465088 \t Time: 22.695999145507812 secs [1127.952104504153 ej/sec]]\n",
      "Epoch [41/50]\t Loss: 0.28175139125389387\n",
      "Starting epoch 42 / 50\n",
      "Step [0/79]\t Loss: 0.2989027500152588 \t Time: 4.156673192977905 secs [6158.771404797345 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.2564329206943512 \t Time: 22.751503467559814 secs [1125.20036473641 ej/sec]]\n",
      "Epoch [42/50]\t Loss: 0.28308192197280596\n",
      "Starting epoch 43 / 50\n",
      "Step [0/79]\t Loss: 0.2866898775100708 \t Time: 4.1610047817230225 secs [6152.360149271289 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.2974959909915924 \t Time: 22.77056622505188 secs [1124.258384573468 ej/sec]]\n",
      "Epoch [43/50]\t Loss: 0.28222739941711666\n",
      "Starting epoch 44 / 50\n",
      "Step [0/79]\t Loss: 0.277240514755249 \t Time: 3.7407591342926025 secs [6843.530706192099 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.3604101240634918 \t Time: 23.12101459503174 secs [1107.2178469841435 ej/sec]]\n",
      "Epoch [44/50]\t Loss: 0.2781949714769291\n",
      "Starting epoch 45 / 50\n",
      "Step [0/79]\t Loss: 0.3521413505077362 \t Time: 2.897658348083496 secs [8834.719944444718 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.23964442312717438 \t Time: 22.93591547012329 secs [1116.1533985136539 ej/sec]]\n",
      "Epoch [45/50]\t Loss: 0.28066887602775914\n",
      "Starting epoch 46 / 50\n",
      "Step [0/79]\t Loss: 0.3536035716533661 \t Time: 3.0851099491119385 secs [8297.921442757353 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.26075488328933716 \t Time: 22.968769550323486 secs [1114.556874451268 ej/sec]]\n",
      "Epoch [46/50]\t Loss: 0.274695050867298\n",
      "Starting epoch 47 / 50\n",
      "Step [0/79]\t Loss: 0.26862674951553345 \t Time: 3.3139913082122803 secs [7724.824122670925 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.22818061709403992 \t Time: 22.99046492576599 secs [1113.5051023395981 ej/sec]]\n",
      "Epoch [47/50]\t Loss: 0.27249640584746493\n",
      "Starting epoch 48 / 50\n",
      "Step [0/79]\t Loss: 0.2663426697254181 \t Time: 4.0788352489471436 secs [6276.301551186223 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.26456841826438904 \t Time: 22.761775493621826 secs [1124.6925797670522 ej/sec]]\n",
      "Epoch [48/50]\t Loss: 0.27126893570905997\n",
      "Starting epoch 49 / 50\n",
      "Step [0/79]\t Loss: 0.23093856871128082 \t Time: 4.000850677490234 secs [6398.6392054149555 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.28840622305870056 \t Time: 22.83106255531311 secs [1121.2793945957858 ej/sec]]\n",
      "Epoch [49/50]\t Loss: 0.2626985975458652\n",
      "Starting epoch 50 / 50\n",
      "Step [0/79]\t Loss: 0.24011343717575073 \t Time: 4.029598712921143 secs [6352.989918800627 ej/sec]]\n",
      "Step [50/79]\t Loss: 0.2797444760799408 \t Time: 22.823335647583008 secs [1121.6590070483865 ej/sec]]\n",
      "Epoch [50/50]\t Loss: 0.2643108628218687\n",
      "Fine tune done and model saved.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "files_test = [hdf5_files_path+s+'.hdf5' for s in samplesvalidation]\n",
    "test_dset = H5IFCBDataset(files_test,classes,classattribute=\"AutoClass\",verbose=1,trainingset=False,transform=val_transform)\n",
    "test_loader = DataLoader(test_dset,batch_size=batch_size,num_workers=num_workers,shuffle=False,pin_memory=True)\n",
    "\n",
    "y_true,y_pred,_,_ = make_preds(model_fe, test_loader, device)\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "\n",
    "y_true,y_pred,_,_ = make_preds(model_ft, test_loader, device)\n",
    "print(accuracy_score(y_true, y_pred))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Loading samples: 100%|██████████| 122/122 [02:23<00:00,  1.18s/it]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8486220711992286\n",
      "0.875965345839535\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('base': conda)",
   "language": "python",
   "name": "python37964bitbaseconda49ce4f6390104a3dba3b01b33b44509f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}